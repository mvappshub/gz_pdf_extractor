// File: config.json
{
  "openrouter": {
    "api_key": "",
    "model": "google/gemini-2.0-flash-exp:free",
    "max_tokens": 4000,
    "temperature": 0.0
  },
  "processing": {
    "input_directory": "C:/gz_projekt/data-for-testing/01",
    "output_directory": "C:/Users/marti/Desktop/gz_pdf_extractor/output-pdf",
    "max_workers": 5,
    "max_file_size_mb": 5000,
    "skip_processed": true
  },
  "pdf": {
    "max_pages": 50,
    "min_text_length": 100,
    "language": "en"
  },
  "advanced": {
    "timeout_seconds": 30,
    "retry_attempts": 3,
    "log_level": "INFO",
    "save_extracted_text": false
  }
}


// File: pdf_extractor.py
# === IMPORTY ===
import os
import json
import argparse
import pdfplumber
import zipfile
import io
import re
import logging
import logging.handlers
import time
import psutil
from typing import List, Optional, Dict, Tuple, Any, Union
from pydantic import BaseModel, Field
from openai import OpenAI
from dotenv import load_dotenv
import concurrent.futures
import threading
from datetime import datetime

# === KONFIGURACE A KONSTANTY ===
# Načtení proměnných prostředí
load_dotenv()

# Výchozí prompt
DEFAULT_PROMPT = """
You are an expert in extracting vinyl record track information from text. 
Extract the following information from the provided text and return it as a JSON object:

- tracks: List of tracks, each track should have:
    - side: The side (e.g., "A", "B")
    - position: The position on the side as integer (e.g., 1, 2)
    - title: The track title
    - duration: Duration in MM:SS format (e.g., "3:45")

Return only valid JSON with no additional text.
Focus on accurate track listing with correct side/position information and precise duration formatting.
"""

# Výchozí konfigurace
DEFAULT_CONFIG = {
  "openrouter": {
    "api_key": os.getenv("OPENROUTER_API_KEY", ""),
    "model": os.getenv("OPENROUTER_MODEL", "google/gemini-2.0-flash-exp:free"),
    "max_tokens": 4000,
    "temperature": 0.0
  },
  "processing": {
    "input_directory": "C:/gz_projekt/data-for-testing",
    "output_directory": "C:/Users/marti/Desktop/pdf extractor/output-pdf",
    "max_workers": 5,
    "max_file_size_mb": 5000,
    "skip_processed": True
  },
  "pdf": {
    "max_pages": 50,
    "min_text_length": 100,
    "language": "en"
  },
  "advanced": {
    "timeout_seconds": 30,
    "retry_attempts": 3,
    "log_level": "INFO",
    "save_extracted_text": False
  }
}

# Zámek pro thread-safe tisk
print_lock = threading.Lock()

# Globální proměnné pro metriky
METRICS = {
    "total_files": 0,
    "processed_files": 0,
    "failed_files": 0,
    "total_processing_time": 0,
    "total_tokens_used": 0,
    "total_response_tokens": 0,
    "successful_parses": 0,
    "failed_parses": 0,
    "start_time": None,
    "end_time": None
}

# === NASTAVENÍ LOGOVÁNÍ ===
def setup_logging(log_level="INFO", log_file=None):
    """Nastavení strukturovaného logování s rotací souborů"""
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level))
    
    # Odstranění existujících handlerů
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Formát pro JSON logování
    formatter = logging.Formatter(
        '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "message": "%(message)s", "module": "%(module)s", "function": "%(funcName)s", "line": %(lineno)d}'
    )
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # File handler s rotací (pokud je specifikován soubor)
    if log_file:
        # Vytvoření adresáře pro logy, pokud neexistuje
        log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir)
            
        file_handler = logging.handlers.RotatingFileHandler(
            log_file, maxBytes=10*1024*1024, backupCount=5
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

# Načtení konfiguračního souboru
def load_config(config_path=None):
    """Načtení konfigurace ze souboru nebo použití výchozí"""
    if config_path and os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # Sloučení s výchozí konfigurací pro doplnění chybějících hodnot
                merged_config = DEFAULT_CONFIG.copy()
                for section, values in config.items():
                    if section in merged_config:
                        merged_config[section].update(values)
                    else:
                        merged_config[section] = values
                return merged_config
        except Exception as e:
            logging.error(f"Failed to load config from {config_path}: {str(e)}")
    
    return DEFAULT_CONFIG.copy()

# === DATOVÉ MODELY (PYDANTIC) ===
class TrackInfo(BaseModel):
    """Informace o skladbě z AI odpovědi"""
    side: str
    position: int
    title: str
    duration: str  # MM:SS formát

class AIResponse(BaseModel):
    """Očekávaná struktura dat z AI API"""
    tracks: List[TrackInfo]

class OutputTrack(BaseModel):
    """Skladba ve výstupním JSON souboru"""
    title: str
    side: str
    position: int
    duration_seconds: int
    duration_formatted: str  # Normalizovaný MM:SS formát

class OutputFileModel(BaseModel):
    """Finální JSON výstup podle šablony"""
    source_type: str = "pdf"
    source_path: str  # Kompletní absolutní cesta k zdrojovému PDF
    path_id: str
    tracks: List[OutputTrack]
    side_durations: Dict[str, str] = Field(default_factory=dict)

# === POMOCNÉ/UTILITY FUNKCE ===
def safe_print(message):
    """Thread-safe printing"""
    with print_lock:
        print(message)

def parse_duration(duration_str: str) -> int:
    """Převede řetězec s dobou trvání na sekundy"""
    if not duration_str:
        return 0
    try:
        parts = duration_str.split(':')
        if len(parts) == 2:  # MM:SS
            return int(parts[0]) * 60 + int(parts[1])
        elif len(parts) == 3:  # HH:MM:SS
            return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
    except:
        pass
    return 0

def format_duration(seconds: int) -> str:
    """Formátuje sekundy jako MM:SS"""
    minutes = seconds // 60
    seconds = seconds % 60
    return f"{minutes:02d}:{seconds:02d}"

def normalize_duration_format(duration_str: str) -> str:
    """
    Standardizuje formát času na MM:SS:
    "7:32" -> "07:32"
    "1:02:35" -> "62:35" (konvertuje hodiny)
    """
    if not duration_str:
        return "00:00"
    
    seconds = parse_duration(duration_str)
    return format_duration(seconds)

def generate_path_based_id(file_path: str) -> str:
    """
    Vytvoří ID na základě cesty k souboru:
    /path/to/Interpret - Album [CAT123] → interpret_album_cat123
    """
    path = file_path.lower()
    # Extrahovat relevantní části
    parts = re.findall(r'[a-z0-9]+', path)
    # Použít posledních 3-5 částí (obvykle nejrelevantnější)
    relevant_parts = parts[-5:] if len(parts) >= 5 else parts
    return "_".join(relevant_parts)

def get_cpu_usage():
    """Získá aktuální využití CPU"""
    try:
        return psutil.cpu_percent(interval=0.1)
    except:
        return 0.0

# === JÁDROVÁ LOGIKA ===
def get_openrouter_client(config):
    """Inicializace OpenRouter.ai klienta s API klíčem z konfigurace"""
    api_key = config["openrouter"]["api_key"]
    if not api_key:
        raise ValueError("OpenRouter API key not configured")
    
    base_url = "https://openrouter.ai/api/v1"
    return OpenAI(base_url=base_url, api_key=api_key)

def load_prompt():
    """Načtení normalizačního promptu ze souboru nebo použití výchozího"""
    prompt_path = os.path.join(os.path.dirname(__file__), "normalize.txt")
    try:
        with open(prompt_path, encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return DEFAULT_PROMPT

def extract_text_from_pdf(pdf_bytes, config):
    """Extrakce textu z PDF souboru z bajtů"""
    text = ""
    max_pages = config["pdf"]["max_pages"]
    
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        pages_to_process = min(len(pdf.pages), max_pages)
        
        for i in range(pages_to_process):
            page = pdf.pages[i]
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    
    return text

def fetch_structured_data_from_ai(text: str, config) -> Tuple[AIResponse, Dict[str, Any]]:
    """
    Extrakce informací o vinylové desce z textu pomocí OpenRouter
    """
    client = get_openrouter_client(config)
    prompt = load_prompt()
    model = config["openrouter"]["model"]
    max_tokens = config["openrouter"]["max_tokens"]
    temperature = config["openrouter"]["temperature"]
    timeout = config["advanced"]["timeout_seconds"]
    retry_attempts = config["advanced"]["retry_attempts"]
    
    logging.info(f"Using OpenRouter with model: {model}")
    
    # Metriky pro tento request
    request_metrics = {
        "prompt_tokens": 0,
        "response_tokens": 0,
        "success": False,
        "attempts": 0
    }
    
    # Logování promptu v DEBUG režimu
    logging.debug(f"AI Prompt: {prompt}")
    logging.debug(f"AI Text (first 500 chars): {text[:500]}...")
    
    last_exception = None
    
    for attempt in range(retry_attempts):
        request_metrics["attempts"] = attempt + 1
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=timeout,
                response_format={"type": "json_object"}
            )
            
            # Získání metrik o tokenech s kontrolou, zda response.usage není None
            if response.usage is not None:
                request_metrics["prompt_tokens"] = response.usage.prompt_tokens
                request_metrics["response_tokens"] = response.usage.completion_tokens
                
                # Aktualizace globálních metrik
                METRICS["total_tokens_used"] += request_metrics["prompt_tokens"]
                METRICS["total_response_tokens"] += request_metrics["response_tokens"]
            else:
                logging.warning("Response usage information is not available")
            
            # Parsování JSON odpovědi
            json_content = response.choices[0].message.content
            if not json_content:
                raise Exception("Received empty response from AI provider")
            
            # Logování odpovědi v DEBUG režimu
            logging.debug(f"AI Response: {json_content}")
            
            # Pokus o parsování JSON a validace Pydantic modelem
            try:
                parsed_data = json.loads(json_content)
                ai_response = AIResponse(**parsed_data)
                
                # Zajištění, že position je integer
                for track in ai_response.tracks:
                    if isinstance(track.position, str):
                        try:
                            track.position = int(track.position)
                        except ValueError:
                            track.position = 1  # Default fallback
                
                request_metrics["success"] = True
                METRICS["successful_parses"] += 1
                
                return ai_response, request_metrics
            except (json.JSONDecodeError, ValueError) as e:
                content_preview = json_content[:200] if json_content else "(empty)"
                last_exception = Exception(f"Failed to parse AI response as JSON: {e}. Response: {content_preview}...")
                logging.warning(f"Parse attempt {attempt + 1} failed: {str(last_exception)}")
                if attempt == retry_attempts - 1:
                    METRICS["failed_parses"] += 1
                    raise last_exception
        
        except Exception as e:
            last_exception = e
            logging.warning(f"API call attempt {attempt + 1} failed: {str(e)}")
            if attempt == retry_attempts - 1:
                METRICS["failed_parses"] += 1
                raise Exception(f"AI API call failed after {retry_attempts} attempts: {str(e)}")
            
            # Čekání před dalším pokusem (exponenciální backoff)
            wait_time = (2 ** attempt) * 1
            logging.info(f"Waiting {wait_time}s before retry...")
            time.sleep(wait_time)
    
    # Tento kód by nikdy neměl být dosažen, ale pro jistotu přidáme vyvolání výjimky
    raise Exception("Unexpected error in fetch_structured_data_from_ai")

# === LOGIKA ZPRACOVÁNÍ A TRANSFORMACE DAT ===
def calculate_side_durations(tracks: List[TrackInfo]) -> Dict[str, str]:
    """Vypočítá celkovou dobu trvání pro každou stranu"""
    side_durations_seconds = {}
    
    for track in tracks:
        side = track.side
        duration_seconds = parse_duration(track.duration)
        
        if side not in side_durations_seconds:
            side_durations_seconds[side] = 0
        side_durations_seconds[side] += duration_seconds
    
    # Formátování výsledků
    return {side: format_duration(total) for side, total in side_durations_seconds.items()}

def transform_ai_response(ai_response: AIResponse, source_path: str) -> dict:
    """
    Transformuje AI response na finální výstupní slovník pro serializaci do JSON
    """
    # Výpočet celkové doby trvání na strany
    side_durations = calculate_side_durations(ai_response.tracks)
    
    # Transformace stop - pořadí podle šablony: title, side, position, duration_seconds, duration_formatted
    output_tracks = []
    for track in ai_response.tracks:
        duration_seconds = parse_duration(track.duration)
        duration_formatted = normalize_duration_format(track.duration)
        
        output_tracks.append(OutputTrack(
            title=track.title,
            side=track.side,
            position=track.position,
            duration_seconds=duration_seconds,
            duration_formatted=duration_formatted
        ))
    
    # Vytvoření výstupního modelu
    output_model = OutputFileModel(
        source_path=source_path,
        path_id=generate_path_based_id(source_path),
        tracks=output_tracks,
        side_durations=side_durations
    )
    
    # Převod na slovník pro serializaci
    return output_model.model_dump()

# === ORCHESTRACE A SOUČĚŽNÉ ZPRACOVÁNÍ ===
def _process_zip_stream(zip_bytes: bytes, abs_path_prefix: str, rel_path_prefix: str) -> List[Tuple[str, str, bytes]]:
    """Rekurzivní zpracování ZIP archivu z bajtového proudu"""
    pdf_files = []
    
    try:
        with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zip_ref:
            for zip_info in zip_ref.infolist():
                if zip_info.filename.startswith('__') or zip_info.filename.startswith('.'):
                    continue
                
                if zip_info.filename.lower().endswith('.pdf'):
                    # PDF v ZIPu
                    with zip_ref.open(zip_info) as pdf_file:
                        zip_abs_path = f"{abs_path_prefix}::{zip_info.filename}"
                        zip_rel_path = f"{rel_path_prefix}::{zip_info.filename}"
                        pdf_files.append((zip_abs_path, zip_rel_path, pdf_file.read()))
                
                elif zip_info.filename.lower().endswith('.zip'):
                    # Vnořený ZIP - extrahujeme a zpracujeme rekurzivně
                    with zip_ref.open(zip_info) as nested_zip_file:
                        nested_zip_bytes = nested_zip_file.read()
                        nested_zip_abs_path = f"{abs_path_prefix}::{zip_info.filename}"
                        nested_zip_rel_path = f"{rel_path_prefix}::{zip_info.filename}"
                        
                        # Rekurzivní zpracování vnořeného ZIPu
                        nested_pdfs = _process_zip_stream(
                            nested_zip_bytes, 
                            nested_zip_abs_path, 
                            nested_zip_rel_path
                        )
                        pdf_files.extend(nested_pdfs)
    except Exception as e:
        logging.error(f"Error processing ZIP stream {rel_path_prefix}: {str(e)}")
    
    return pdf_files

def collect_pdf_sources(config) -> List[Tuple[str, str, bytes]]:
    """Sbírá všechny PDF soubory z adresáře včetně ZIP archivů
    
    Returns:
        List of tuples: (absolute_path, relative_path, pdf_bytes)
    """
    pdf_files = []
    input_dir = config["processing"]["input_directory"]
    max_file_size_mb = config["processing"]["max_file_size_mb"]
    max_file_size_bytes = max_file_size_mb * 1024 * 1024
    
    logging.info(f"Scanning directory: {input_dir}")
    
    for root, dirs, files in os.walk(input_dir):
        for file in files:
            file_path = os.path.join(root, file)
            abs_path = os.path.abspath(file_path)
            rel_path = os.path.relpath(file_path, input_dir)
            
            # Kontrola velikosti souboru
            try:
                file_size = os.path.getsize(file_path)
                if file_size > max_file_size_bytes:
                    logging.warning(f"Skipping file {rel_path} due to size ({file_size/1024/1024:.2f}MB > {max_file_size_mb}MB)")
                    continue
            except Exception as e:
                logging.warning(f"Could not get size of {rel_path}: {str(e)}")
                continue
            
            if file.lower().endswith('.pdf'):
                # Přímé PDF soubory
                try:
                    with open(file_path, 'rb') as f:
                        pdf_files.append((abs_path, rel_path, f.read()))
                except Exception as e:
                    logging.error(f"Error reading PDF file {rel_path}: {str(e)}")
            
            elif file.lower().endswith('.zip'):
                # ZIP archivy - zpracování pomocí _process_zip_stream
                try:
                    with open(file_path, 'rb') as f:
                        zip_bytes = f.read()
                        zip_pdfs = _process_zip_stream(zip_bytes, abs_path, rel_path)
                        pdf_files.extend(zip_pdfs)
                except Exception as e:
                    logging.error(f"Error processing ZIP file {rel_path}: {str(e)}")
    
    logging.info(f"Found {len(pdf_files)} PDF files to process")
    return pdf_files

def process_single_pdf(pdf_data: Tuple[str, str, bytes], config, output_dir, processed_files=None) -> Optional[Dict[str, Any]]:
    """Zpracování jednoho PDF souboru"""
    abs_path, pdf_id, pdf_bytes = pdf_data
    start_time = time.time()
    
    # Vytvoření bezpečného názvu souboru z cesty
    safe_filename = pdf_id.replace('::', '_').replace('/', '_').replace('\\', '_')
    output_file = os.path.splitext(safe_filename)[0] + ".json"
    output_path = os.path.join(output_dir, output_file)
    
    # Kontrola, zda byl soubor již zpracován
    if processed_files is not None and output_path in processed_files:
        logging.info(f"Skipping already processed file: {pdf_id}")
        return None
    
    logging.info(f"Processing: {pdf_id}")
    logging.debug(f"Source path: {abs_path}")
    
    try:
        # Extrakce textu z PDF
        text = extract_text_from_pdf(pdf_bytes, config)
        
        if not text.strip() or len(text.strip()) < config["pdf"]["min_text_length"]:
            logging.warning(f"Skipping {pdf_id}: Not enough text extracted ({len(text.strip())} chars)")
            return None
        
        # Uložení extrahovaného textu pro debugování (pokud je povoleno)
        if config["advanced"]["save_extracted_text"]:
            debug_file = os.path.splitext(safe_filename)[0] + "_extracted_text.txt"
            debug_path = os.path.join(output_dir, debug_file)
            with open(debug_path, 'w', encoding='utf-8') as f:
                f.write(text)
        
        # Extrakce dat pomocí AI
        ai_response, request_metrics = fetch_structured_data_from_ai(text, config)
        
        # Transformace dat na výstupní formát
        output_data = transform_ai_response(ai_response, abs_path)
        
        # Uložení výstupního JSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        processing_time = time.time() - start_time
        
        # Aktualizace metrik
        METRICS["processed_files"] += 1
        METRICS["total_processing_time"] += processing_time
        
        logging.info(f"Success: Data saved to {output_path}")
        logging.debug(f"Path ID: {output_data['path_id']}")
        logging.debug(f"Tracks: {len(output_data['tracks'])} with total duration data")
        logging.debug(f"Processing time: {processing_time:.2f}s")
        logging.debug(f"Request metrics: {json.dumps(request_metrics)}")
        
        return {
            "path": output_path,
            "success": True,
            "processing_time": processing_time,
            "tracks_count": len(output_data["tracks"]),
            "request_metrics": request_metrics
        }
        
    except Exception as e:
        processing_time = time.time() - start_time
        METRICS["failed_files"] += 1
        METRICS["total_processing_time"] += processing_time
        
        logging.error(f"Error processing {pdf_id}: {str(e)}")
        
        # Uložení chybového souboru
        error_file = os.path.splitext(safe_filename)[0] + "_error.txt"
        error_path = os.path.join(output_dir, error_file)
        try:
            with open(error_path, 'w', encoding='utf-8') as f:
                f.write(f"Error: {str(e)}\n\n")
                f.write(f"Source path: {abs_path}\n\n")
                f.write(f"Processing time: {processing_time:.2f}s\n\n")
                # Získání textu pro chybový soubor
                try:
                    text = extract_text_from_pdf(pdf_bytes, config)
                    f.write(f"Extracted text:\n{text[:2000]}...")  # Prvních 2000 znaků textu
                except:
                    f.write("Could not extract text for error report.")
            logging.info(f"Error details saved to {error_path}")
        except Exception as save_error:
            logging.error(f"Could not save error file: {str(save_error)}")
        
        return {
            "path": pdf_id,
            "success": False,
            "processing_time": processing_time,
            "error": str(e)
        }

def run_processing_pipeline(config):
    """
    Zpracování všech PDF souborů v zadaném adresáři paralelně
    """
    input_dir = config["processing"]["input_directory"]
    output_dir = config["processing"]["output_directory"]
    max_workers = config["processing"]["max_workers"]
    skip_processed = config["processing"]["skip_processed"]
    
    if not os.path.isdir(input_dir):
        logging.error(f"Input directory '{input_dir}' does not exist.")
        return
    
    # Vytvoření výstupního adresáře, pokud neexistuje
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # Sada pro sledování již zpracovaných souborů
    processed_files = set()
    if skip_processed:
        for root, _, files in os.walk(output_dir):
            for file in files:
                if file.endswith('.json'):
                    processed_files.add(os.path.join(root, file))
        logging.info(f"Found {len(processed_files)} already processed files to skip")
    
    # Sběr všech PDF souborů
    logging.info("Collecting PDF files...")
    start_time = time.time()
    pdf_files = collect_pdf_sources(config)
    collection_time = time.time() - start_time
    
    if not pdf_files:
        logging.warning(f"No PDF files found in '{input_dir}'.")
        return
    
    METRICS["total_files"] = len(pdf_files)
    METRICS["start_time"] = datetime.now()
    
    logging.info(f"Found {len(pdf_files)} PDF files to process (collection took {collection_time:.2f} seconds).")
    logging.info(f"Processing with {max_workers} concurrent workers...")
    
    # Paralelní zpracování pomocí ThreadPoolExecutor
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Vytvoření úloh pro každé PDF
        futures = [executor.submit(process_single_pdf, pdf_data, config, output_dir, processed_files) for pdf_data in pdf_files]
        
        # Sledování průběhu
        completed = 0
        for future in concurrent.futures.as_completed(futures):
            try:
                result = future.result()
                if result:
                    results.append(result)
                completed += 1
                
                # Aktualizace průběhu
                progress = (completed / len(pdf_files)) * 100
                cpu_usage = get_cpu_usage()
                
                logging.info(f"Progress: {completed}/{len(pdf_files)} ({progress:.1f}%), CPU: {cpu_usage:.1f}%")
                
            except Exception as e:
                logging.error(f"Unexpected error in worker: {str(e)}")
    
    METRICS["end_time"] = datetime.now()
    
    # Výpočet celkových metrik
    total_time = (METRICS["end_time"] - METRICS["start_time"]).total_seconds()
    avg_time_per_file = METRICS["total_processing_time"] / max(METRICS["processed_files"], 1)
    
    logging.info(f"\n=== Processing Summary ===")
    logging.info(f"Successfully processed: {METRICS['processed_files']}/{METRICS['total_files']} files")
    logging.info(f"Failed: {METRICS['failed_files']} files")
    logging.info(f"Total time: {total_time:.2f} seconds")
    logging.info(f"Average time per file: {avg_time_per_file:.2f} seconds")
    logging.info(f"Total tokens used: {METRICS['total_tokens_used']}")
    logging.info(f"Total response tokens: {METRICS['total_response_tokens']}")
    logging.info(f"Successful AI parses: {METRICS['successful_parses']}")
    logging.info(f"Failed AI parses: {METRICS['failed_parses']}")
    
    # Uložení metrik do souboru
    metrics_file = os.path.join(output_dir, "processing_metrics.json")
    try:
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "config": config,
                "metrics": METRICS,
                "results": results
            }, f, indent=2, ensure_ascii=False)
        logging.info(f"Metrics saved to {metrics_file}")
    except Exception as e:
        logging.error(f"Could not save metrics: {str(e)}")

# === HLAVNÍ SPUŠTĚCÍ BLOK ===
def main():
    parser = argparse.ArgumentParser(description='Extract vinyl record information from PDF files using OpenRouter AI.')
    parser.add_argument('--config', '-c', help='Path to configuration file')
    parser.add_argument('--log-level', '-l', default="INFO", 
                        help='Logging level (DEBUG, INFO, WARNING, ERROR)')
    parser.add_argument('--log-file', help='Path to log file')
    
    args = parser.parse_args()
    
    # Načtení konfigurace
    config = load_config(args.config)
    
    # Nastavení logování
    logger = setup_logging(args.log_level, args.log_file)
    
    logging.info("=== Cue-AI Vinyl Record Extractor ===")
    logging.info(f"Input directory: {config['processing']['input_directory']}")
    logging.info(f"Output directory: {config['processing']['output_directory']}")
    logging.info(f"Concurrent workers: {config['processing']['max_workers']}")
    logging.info(f"OpenRouter model: {config['openrouter']['model']}")
    logging.info("=====================================")
    
    try:
        run_processing_pipeline(config)
        logging.info("\nProcessing complete!")
    except Exception as e:
        logging.error(f"Fatal error: {str(e)}")
        raise

if __name__ == "__main__":
    main()

// File: run_pdf_extractor.bat
@echo off
echo Spoustim extrakci z PDF souboru...

:: 1. Aktivace virtualniho prostredi
call venv\Scripts\activate.bat

:: 2. Spusteni skriptu s konfiguracnim souborem
::    ZDE MUZETE SNADNO UPRAVIT CESTY V SOUBORU config.json
python pdf_extractor.py --config config.json

:: 3. Deaktivace virtualniho prostredi
call deactivate

echo.
echo Zpracovani dokonceno. Stisknete klavesu pro ukonceni.
pause